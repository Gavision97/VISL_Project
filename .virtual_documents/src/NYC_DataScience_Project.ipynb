





import pandas as pd
import seaborn as sns
import numpy as np

import os
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_log_error

import warnings
warnings.filterwarnings('ignore')


def PRINT(msg):
    print('--------------------------------------------------------------------------------------------------')
    print(msg)
    print('--------------------------------------------------------------------------------------------------')








ny_o3_2016_df = pd.read_csv(os.path.join('datasets', 'pollution_datasets', 'ny_ozone_2016.csv'))
ny_o3_2016_df.head()


ny_o3_2016_df.columns


ny_o3_2016_df.head(3)


ny_o3_2016_df['County'].unique()








def preprocess_pollutant(pollutant) -> pd.DataFrame():
    pollutant_dict = {}
    years = ['2016', '2018', '2020', '2021']
    for year in years: 
        # load current dataset, select relevant columns & rename them
        curr_df = pd.read_csv(os.path.join('datasets', 'pollution_datasets', f'ny_{pollutant.lower()}_{year}.csv'))
        curr_df = curr_df[['Date',f'{pollutant}', 'AQI', 'County', 'Site Latitude', 'Site Longitude']].copy()
        curr_df['Site Latitude'] = curr_df['Site Latitude'].round(3)
        curr_df['Site Longitude'] = curr_df['Site Longitude'].round(3)

        # group by date and compute average of pollutant and other numeric values
        curr_df['Date'] = pd.to_datetime(curr_df['Date'])
        #curr_df = curr_df.groupby('Date', 'Site Latitude', 'Site Longitude')[[f'{pollutant}', 'AQI']].mean().reset_index()
        #curr_df.rename(columns={f'{pollutant}': f'avg_{pollutant}'}, inplace=True)
        
        pollutant_dict[year] = curr_df
    return pollutant_dict


o3_dict = preprocess_pollutant('Ozone')
no2_dict = preprocess_pollutant('NO2')
pm2d5_dict = preprocess_pollutant('PM2.5')
co_dict = preprocess_pollutant('CO')


o3_dict['2021']





o3_df = pd.concat([o3_dict['2016'], o3_dict['2018'], o3_dict['2020'], o3_dict['2021']], axis=0).reset_index(drop=True)
no2_df = pd.concat([no2_dict['2016'], no2_dict['2018'], no2_dict['2020'], no2_dict['2021']], axis=0).reset_index(drop=True)
pm2d5_df = pd.concat([pm2d5_dict['2016'], pm2d5_dict['2018'], pm2d5_dict['2020'], pm2d5_dict['2021']], axis=0).reset_index(drop=True)
co_df = pd.concat([co_dict['2016'], co_dict['2018'], co_dict['2020'], co_dict['2021']], axis=0).reset_index(drop=True)


nyc_pollutant_df = o3_df.merge(no2_df, how='inner', on=['Date', 'County', 'Site Latitude', 'Site Longitude'], suffixes=('_o3', '_no2'))
nyc_pollutant_df = nyc_pollutant_df.merge(pm2d5_df, how='inner', on=['Date', 'County', 'Site Latitude', 'Site Longitude'], suffixes=('_no2', '_pm2.5'))
nyc_pollutant_df = nyc_pollutant_df.merge(co_df, how='inner', on=['Date', 'County', 'Site Latitude', 'Site Longitude'], suffixes=('_pm2.5', '_co'))


nyc_pollutant_df['County'].unique()





nyc_pollutant_df.columns


nyc_pollutant_df = nyc_pollutant_df[['Date', 'County', 'Site Latitude', 'Site Longitude','Ozone', 'AQI_o3','NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO', 'AQI_co']]
PRINT(f'Dataset shape -> {nyc_pollutant_df.shape}')
nyc_pollutant_df.head()


PRINT(f'Number of null values in nyc_pollutant_df -> {nyc_pollutant_df.isna().sum().sum()}')





ny_weather_df = pd.read_csv(os.path.join('datasets', 'NYC_Weather_2016_2022.csv'))
ny_weather_df.head(3)








ny_weather_df.isnull().sum()





ny_weather_df = ny_weather_df.dropna()
PRINT(f'Number of null values -> {ny_weather_df.isnull().sum().sum()}')





# seperate the time from the date column in order to group by the date and calculate average values for each day
ny_weather_df['time'] = pd.to_datetime(ny_weather_df['time'])
ny_weather_df['day'] = ny_weather_df['time'].dt.date

# group by 'day' and take the mean of all numeric columns
daily_avg_df = ny_weather_df.groupby('day').mean().reset_index()
daily_avg_df.drop(columns=['time'], inplace=True)
daily_avg_df.rename(columns={'day': 'Date'}, inplace=True)
daily_avg_df['Date'] = pd.to_datetime(daily_avg_df['Date'])


# 2896


daily_avg_df


merged_df = daily_avg_df.merge(nyc_pollutant_df, how='inner', on='Date')
PRINT(f'Merged dataset shape -> {merged_df.shape}')


merged_df.head(3)


PRINT(f'Columns -> {list(merged_df.columns)}')


merged_df = merged_df[['Date',  'County', 'Site Latitude',
                       'Site Longitude', 'Ozone', 'AQI_o3', 'NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO', 'AQI_co', 'temperature_2m (°C)', 'precipitation (mm)', 'rain (mm)', 'cloudcover (%)',
                       'cloudcover_low (%)', 'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)']]
merged_df.head()





merged_df.head()








numeric_cols = merged_df.loc[:, ['cloudcover (%)', 'cloudcover_low (%)',
                     'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'Ozone', 'AQI_o3', 'NO2', 'AQI_no2','PM2.5', 'AQI_pm2.5','CO', 'AQI_co'
                    ]].select_dtypes(include='number').columns # all numeric features without the lables, date, logtitune and latitude

# determine grid size
n_cols = 4 
n_rows = -(-len(numeric_cols) // n_cols)  


fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
axes = axes.flatten()

# plot each histogram
for i, col in enumerate(numeric_cols):
    sns.histplot(merged_df[col], kde=True, ax=axes[i])
    axes[i].set_title(col)

# hide unused subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()


# Only include the columns you want to normalize (excluding temp & rain)
cols_to_normalize = ['cloudcover (%)', 'cloudcover_low (%)',
                     'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'avg_Ozone', 'AQI_o3', 'Site Latitude_o3', 'Site Longitude_o3', 'avg_NO2', 'AQI_no2', 'Site Latitude_no2',
                     'Site Longitude_no2', 'avg_PM2.5', 'AQI_pm2.5', 'Site Latitude_pm2.5', 'Site Longitude_pm2.5',
                     'avg_CO', 'AQI_co', 'Site Latitude_co', 'Site Longitude_co'
                    ]

scaler = MinMaxScaler()
merged_df[cols_to_normalize] = scaler.fit_transform(merged_df[cols_to_normalize])


#scaler = StandardScaler()
scaler = MinMaxScaler()

cols_to_normalize = ['cloudcover (%)', 'cloudcover_low (%)',
                     'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'Ozone', 'AQI_o3', 'NO2', 'AQI_no2','PM2.5', 'AQI_pm2.5','CO', 'AQI_co'
                    ]

merged_df[cols_to_normalize] = scaler.fit_transform(merged_df[cols_to_normalize])
merged_df.tail()





corr_df = merged_df[['temperature_2m (°C)', 'precipitation (mm)', 'rain (mm)', 'cloudcover (%)',
                     'cloudcover_low (%)', 'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'Site Latitude', 'Site Longitude', 'Ozone', 'AQI_o3', 'NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO', 'AQI_co']]


plt.figure(figsize=(12, 8))
sns.heatmap(corr_df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Between Weather Conditions and Pollutants')
plt.tight_layout()
plt.show()









plt.figure(figsize=(10, 6))
sns.lmplot(x='Ozone', y='temperature_2m (°C)', data=merged_df,  scatter_kws={'s': 7})
plt.title('How Ozone Affects Temperature')
plt.xlabel('Ozone')
plt.ylabel('Temperature')
plt.tight_layout()
plt.show()









pollutants = ['Ozone', 'NO2', 'PM2.5', 'CO']

df = merged_df.copy()
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
df['Year'] = df.index.year

# group by year and take mean
yearly_avg = df.groupby('Year')[pollutants].mean().reset_index()
yearly_avg





df = merged_df.copy()
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
df['Year'] = df.index.year

pollutants = ['Ozone', 'NO2', 'PM2.5', 'CO']
df_long = df.melt(id_vars='Year', value_vars=pollutants, 
                  var_name='Pollutant', value_name='Concentration')

plt.figure(figsize=(16, 10))
sns.boxplot(data=df_long, x='Year', y='Concentration', hue='Pollutant')
plt.title('Yearly Distribution of Pollutant Levels in NYC')
plt.xlabel('Year')
plt.ylabel('Concentration')
plt.legend(title='Pollutant')
plt.tight_layout()
plt.show()








melted_avg = yearly_avg.melt(id_vars='Year', var_name='Variable', value_name='Average')


plt.figure(figsize=(14, 6))
sns.barplot(data=melted_avg, x='Year', y='Average', hue='Variable')
plt.title('Yearly Averages of Pollutants and Climate Variables')
plt.ylabel('Average Value')
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.legend(title='Variable', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()









PRINT(f'Model current columns -> {merged_df.columns}')


merged_df.shape





merged_df['Date'] = pd.to_datetime(merged_df['Date'])  # Ensure it's datetime
merged_df['month'] = merged_df['Date'].dt.month  # extract month (1-12)

# one-hot encode the month
month_dummies = pd.get_dummies(merged_df['month'], prefix='', prefix_sep='').astype(int)

#  rename columns to be consistent (e.g., Jan, Feb, ...) & drop date and month columns
month_dummies.columns = [f'month_{i:02d}' for i in range(1, 13)]
merged_df.drop(columns=['Date', 'month'], inplace=True)


merged_df = pd.concat([merged_df, month_dummies], axis=1)


merged_df





merged_df.drop(columns = {'precipitation (mm)', 'rain (mm)'}, inplace=True)


cols = [col for col in merged_df.columns if col not in ['temperature_2m (°C)']]
cols += ['temperature_2m (°C)']
merged_df = merged_df[cols]
merged_df.rename(columns = {'temperature_2m (°C)': 'temperature'}, inplace=True)


merged_df.to_csv(os.path.join('datasets', 'final_df.csv'), index=False)








from imports import *
from VISL import *


final_df = pd.read_csv(os.path.join('datasets', 'final_df.csv'))
df_columns = list(final_df.columns)
feature_cols = df_columns[:-1]
label_cols = ['temperature']








model = VISL()

train_ds, val_ds, test_ds = prepare_data(final_df, feature_cols, label_cols)

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=32)
test_loader = DataLoader(test_ds, batch_size=32)

# train & evaluate model performance
trained_model = train_model(model, train_loader, val_loader, epochs=200)
preds, actuals = evaluate_model(trained_model, test_loader)






def train_baseline_models(df, feature_cols, label_cols):
    X = df[feature_cols].values
    y = df[label_cols].values

    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)
    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=True, random_state=42)

    # initialize baseline bodels
    models = {
        "Linear Regression": LinearRegression(),
        "Random Forest": RandomForestRegressor(n_estimators=50),
    }

    PRINT("Baseline Model Performance (Test Set):")
    for name, model in models.items():
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        
        mae = mean_absolute_error(y_test, preds)
        r2 = r2_score(y_test, preds)
        evs = explained_variance_score(y_test, preds)
        
        print(f"{name}: MAE = {mae:.4f}, R²: {r2:.4f}, EVS: {evs:.4f}")



train_baseline_models(final_df, feature_cols, label_cols)






