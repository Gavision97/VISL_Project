





from imports import *


def PRINT(msg):
    print('--------------------------------------------------------------------------------------------------')
    print(msg)
    print('--------------------------------------------------------------------------------------------------')





ny_o3_2016_df = pd.read_csv(os.path.join('datasets', 'pollution_datasets', 'ny_ozone_2016.csv'))
ny_o3_2016_df.head()


ny_o3_2016_df['County'].unique()





def preprocess_pollutant(pollutant) -> pd.DataFrame():
    pollutant_dict = {}
    years = ['2016', '2018', '2020', '2021']
    for year in years: 
        # load current dataset, select relevant columns & rename them
        curr_df = pd.read_csv(os.path.join('datasets', 'pollution_datasets', f'ny_{pollutant.lower()}_{year}.csv'))
        curr_df = curr_df[['Date',f'{pollutant}', 'AQI', 'County', 'Site Latitude', 'Site Longitude']].copy()
        curr_df['Site Latitude'] = curr_df['Site Latitude'].round(3)
        curr_df['Site Longitude'] = curr_df['Site Longitude'].round(3)
        curr_df['Date'] = pd.to_datetime(curr_df['Date'])
        pollutant_dict[year] = curr_df
        
    return pollutant_dict


o3_dict = preprocess_pollutant('Ozone')
no2_dict = preprocess_pollutant('NO2')
pm2d5_dict = preprocess_pollutant('PM2.5')
co_dict = preprocess_pollutant('CO')


o3_dict['2021']





o3_df = pd.concat([o3_dict['2016'], o3_dict['2018'], o3_dict['2020'], o3_dict['2021']], axis=0).reset_index(drop=True)
no2_df = pd.concat([no2_dict['2016'], no2_dict['2018'], no2_dict['2020'], no2_dict['2021']], axis=0).reset_index(drop=True)
pm2d5_df = pd.concat([pm2d5_dict['2016'], pm2d5_dict['2018'], pm2d5_dict['2020'], pm2d5_dict['2021']], axis=0).reset_index(drop=True)
co_df = pd.concat([co_dict['2016'], co_dict['2018'], co_dict['2020'], co_dict['2021']], axis=0).reset_index(drop=True)


nyc_pollutant_df = o3_df.merge(no2_df, how='inner', on=['Date', 'County', 'Site Latitude', 'Site Longitude'], suffixes=('_o3', '_no2'))
nyc_pollutant_df = nyc_pollutant_df.merge(pm2d5_df, how='inner', on=['Date', 'County', 'Site Latitude', 'Site Longitude'], suffixes=('_no2', '_pm2.5'))
nyc_pollutant_df = nyc_pollutant_df.merge(co_df, how='inner', on=['Date', 'County', 'Site Latitude', 'Site Longitude'], suffixes=('_pm2.5', '_co'))


nyc_pollutant_df['County'].unique()





nyc_pollutant_df.columns


nyc_pollutant_df = nyc_pollutant_df[['Date', 'County', 'Site Latitude', 'Site Longitude','Ozone', 'AQI_o3','NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO', 'AQI_co']]
PRINT(f'Dataset shape -> {nyc_pollutant_df.shape}')
nyc_pollutant_df.head()


PRINT(f'Number of null values in nyc_pollutant_df -> {nyc_pollutant_df.isna().sum().sum()}')





ny_weather_df = pd.read_csv(os.path.join('datasets', 'NYC_Weather_2016_2022.csv'))
ny_weather_df.head(3)








ny_weather_df.isnull().sum()





ny_weather_df = ny_weather_df.dropna()
PRINT(f'Number of null values -> {ny_weather_df.isnull().sum().sum()}')





# seperate the time from the date column in order to group by the date and calculate average values for each day
ny_weather_df['time'] = pd.to_datetime(ny_weather_df['time'])
ny_weather_df['day'] = ny_weather_df['time'].dt.date

# group by 'day' and take the mean of all numeric columns
daily_avg_df = ny_weather_df.groupby('day').mean().reset_index()
daily_avg_df.drop(columns=['time'], inplace=True)
daily_avg_df.rename(columns={'day': 'Date'}, inplace=True)
daily_avg_df['Date'] = pd.to_datetime(daily_avg_df['Date'])


# 2896


daily_avg_df.head(3)


merged_df = daily_avg_df.merge(nyc_pollutant_df, how='inner', on='Date')
PRINT(f'Merged dataset shape -> {merged_df.shape}')


PRINT(f'Columns -> {list(merged_df.columns)}')


merged_df = merged_df[['Date',  'County', 'Site Latitude',
                       'Site Longitude', 'Ozone', 'AQI_o3', 'NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO', 'AQI_co', 'temperature_2m (°C)', 'precipitation (mm)', 'rain (mm)', 'cloudcover (%)',
                       'cloudcover_low (%)', 'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)']]
merged_df.head()








numeric_cols = merged_df.loc[:, ['cloudcover (%)', 'cloudcover_low (%)',
                     'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'Ozone', 'AQI_o3', 'NO2', 'AQI_no2','PM2.5', 'AQI_pm2.5','CO', 'AQI_co'
                    ]].select_dtypes(include='number').columns # all numeric features without the lables, date, logtitune and latitude

# determine grid size
n_cols = 4 
n_rows = -(-len(numeric_cols) // n_cols)  


fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
axes = axes.flatten()

# plot each histogram
for i, col in enumerate(numeric_cols):
    sns.histplot(merged_df[col], kde=True, ax=axes[i])
    axes[i].set_title(col)

# hide unused subplots
for j in range(i + 1, len(axes)):
    axes[j].set_visible(False)

plt.tight_layout()
plt.show()





scaler = MinMaxScaler()

cols_to_normalize = ['cloudcover (%)', 'cloudcover_low (%)',
                     'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'Ozone', 'AQI_o3', 'NO2', 'AQI_no2','PM2.5', 'AQI_pm2.5','CO', 'AQI_co'
                    ]

merged_df[cols_to_normalize] = scaler.fit_transform(merged_df[cols_to_normalize])
merged_df.tail()





corr_df = merged_df[['temperature_2m (°C)', 'precipitation (mm)', 'rain (mm)', 'cloudcover (%)',
                     'cloudcover_low (%)', 'cloudcover_mid (%)', 'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
                     'Site Latitude', 'Site Longitude', 'Ozone', 'AQI_o3', 'NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO', 'AQI_co']]


plt.figure(figsize=(12, 8))
sns.heatmap(corr_df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Between Weather Conditions and Pollutants')
plt.tight_layout()
plt.show()









plt.figure(figsize=(10, 6))
sns.lmplot(x='Ozone', y='temperature_2m (°C)', data=merged_df,  scatter_kws={'s': 7})
plt.title('How Ozone Affects Temperature')
plt.xlabel('Ozone')
plt.ylabel('Temperature')
plt.tight_layout()
plt.show()









pollutants = ['Ozone', 'NO2', 'PM2.5', 'CO']

df = merged_df.copy()
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
df['Year'] = df.index.year

# group by year and take mean
yearly_avg = df.groupby('Year')[pollutants].mean().reset_index()
yearly_avg





df = merged_df.copy()
df['Date'] = pd.to_datetime(df['Date'])
df.set_index('Date', inplace=True)
df['Year'] = df.index.year

pollutants = ['Ozone', 'NO2', 'PM2.5', 'CO']
df_long = df.melt(id_vars='Year', value_vars=pollutants, 
                  var_name='Pollutant', value_name='Concentration')

plt.figure(figsize=(16, 10))
sns.boxplot(data=df_long, x='Year', y='Concentration', hue='Pollutant')
plt.title('Yearly Distribution of Pollutant Levels in NYC')
plt.xlabel('Year')
plt.ylabel('Concentration')
plt.legend(title='Pollutant')
plt.tight_layout()
plt.show()








melted_avg = yearly_avg.melt(id_vars='Year', var_name='Variable', value_name='Average')


plt.figure(figsize=(14, 6))
sns.barplot(data=melted_avg, x='Year', y='Average', hue='Variable')
plt.title('Yearly Averages of Pollutants and Climate Variables')
plt.ylabel('Average Value')
plt.xlabel('Year')
plt.xticks(rotation=45)
plt.legend(title='Variable', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()









merged_df.shape


PRINT(f'Model current columns -> {merged_df.columns}')





merged_df['Date'] = pd.to_datetime(merged_df['Date'])


merged_df['month'] = merged_df['Date'].dt.month
merged_df['year'] = merged_df['Date'].dt.year
merged_df['weekday'] = merged_df['Date'].dt.day_name()  # e.g., 'Sunday'

# one-hot encode month
month_dummies = pd.get_dummies(merged_df['month'], prefix='', prefix_sep='').astype(int)
month_dummies.columns = [f'month_{i:02d}' for i in range(1, 13)]

# one-hot encode year
year_dummies = pd.get_dummies(merged_df['year'], prefix='year', prefix_sep='_').astype(int)

# one-hot encode weekday (Sunday first)
weekday_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
weekday_dummies = pd.get_dummies(merged_df['weekday'], dtype=int)
weekday_dummies = weekday_dummies.reindex(columns=weekday_order, fill_value=0)
weekday_dummies.columns = [f'day_{day[:3].lower()}' for day in weekday_dummies.columns]

# Drop original date-related columns
merged_df.drop(columns=['Date', 'month', 'year', 'weekday'], inplace=True)

# Combine all one-hot encoded columns with the original DataFrame
merged_df = pd.concat([merged_df, month_dummies, year_dummies, weekday_dummies], axis=1)


merged_df





county_dummies = pd.get_dummies(merged_df['County'], prefix='', prefix_sep='').astype(int)
county_dummies.columns = [f'county_{i:02d}' for i in range(1, 3)]
merged_df.drop(columns=['County'], inplace=True)

merged_df = pd.concat([merged_df, county_dummies], axis=1)


merged_df.head(2)


merged_df.columns





coords = merged_df[['Site Latitude', 'Site Longitude']]
kmeans = KMeans(n_clusters=2, random_state=42).fit(coords)
merged_df['cluster'] = kmeans.labels_


merged_df['cluster'].unique()


merged_df.drop(columns=coords.columns, inplace=True)
merged_df.head()


final_df = merged_df[['Ozone', 'AQI_o3', 'NO2', 'AQI_no2', 'PM2.5', 'AQI_pm2.5', 'CO',
       'AQI_co', 'cluster', 'cloudcover (%)', 'cloudcover_low (%)', 'cloudcover_mid (%)',
       'cloudcover_high (%)', 'windspeed_10m (km/h)', 'winddirection_10m (°)',
       'month_01', 'month_02', 'month_03', 'month_04', 'month_05', 'month_06',
       'month_07', 'month_08', 'month_09', 'month_10', 'month_11', 'month_12',
        'year_2018', 'year_2020', 'year_2021', 'day_sun', 'day_mon', 'day_tue',
       'day_wed', 'day_thu', 'day_fri', 'day_sat',
       'county_01', 'county_02', 'precipitation (mm)', 'rain (mm)', 'temperature_2m (°C)']]
final_df.head(2)


final_df.rename(columns={'precipitation (mm)': 'precipitation', 'rain (mm)': 'rain', 'temperature_2m (°C)': 'temperature'}, inplace=True)


final_df.shape


final_df.to_csv(os.path.join('datasets', 'final_df.csv'), index=False)








from VISL import *


final_df = pd.read_csv(os.path.join('datasets', 'final_df.csv'))
df_columns = list(final_df.columns)
feature_cols = df_columns[:-3] # dont include precipitation, rain and temperature
label_cols = ['temperature']











visl_res_dict = {'mae': [], 'r2': [], 'evs': []}

PRINT("Start training VISL models 10 times ...")
for i in range(10):
    model = VISL()
    
    train_ds, val_ds, test_ds = prepare_data(final_df, feature_cols, label_cols)
    
    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=32, shuffle=True)
    
    # train & evaluate model performance
    trained_model = train_model(model, train_loader, val_loader, epochs=150, lr=1e-3, weight_decay=1e-5)
    mae, r2, evs = evaluate_model(trained_model, test_loader)
    
    visl_res_dict['mae'].append(mae)
    visl_res_dict['r2'].append(r2)
    visl_res_dict['evs'].append(evs)


visl_evaluation_statistics = {'mae': [], 'r2': [], 'evs': []}

for k, v in visl_res_dict.items():
    # calculate metric mean, variance and std
    mean = np.mean(v)
    variance = np.var(v)
    std = np.std(v)

    visl_evaluation_statistics[k] = [round(float(x), 5) for x in [mean, variance, std]]


print(visl_evaluation_statistics)





def train_baseline_models(df, feature_cols, label_cols):    
    lnr_res_dict = {'mae': [], 'r2': [], 'evs': []}
    rf_res_dict = {'mae': [], 'r2': [], 'evs': []}
    
    X = df[feature_cols].values
    y = df[label_cols].values

    for i in range(10):
        
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
    
        # initialize baseline bodels
        models = {
            "Linear Regression": LinearRegression(),
            "Random Forest": RandomForestRegressor(n_estimators=60),
        }
        for name, model in models.items():
            model.fit(X_train, y_train)
            preds = model.predict(X_test)
            
            mae = round(mean_absolute_error(y_test, preds), 4)
            r2 = round(r2_score(y_test, preds), 4)
            evs = round(explained_variance_score(y_test, preds), 4)
            
            #print(f"{name}: MAE = {mae:.4f}, R²: {r2:.4f}, EVS: {evs:.4f}")
            if name == "Linear Regression":
                lnr_res_dict['mae'].append(mae)
                lnr_res_dict['r2'].append(r2)
                lnr_res_dict['evs'].append(evs)
            else:
                rf_res_dict['mae'].append(mae)
                rf_res_dict['r2'].append(r2)
                rf_res_dict['evs'].append(evs)
    
    return lnr_res_dict, rf_res_dict

# Compute stats for both models
def compute_stats(metrics_dict):
    stats = {}
    for k, v in metrics_dict.items():
        mean = round(float(np.mean(v)), 5)
        variance = round(float(np.var(v)), 5)
        std = round(float(np.std(v)), 5)
        stats[k] = [mean, variance, std]
    return stats


lnr_res_dict, rf_res_dict = train_baseline_models(final_df, feature_cols, label_cols)


lnr_stats = compute_stats(lnr_res_dict)
rf_stats = compute_stats(rf_res_dict)


def create_long_df(data_dict, model_name):
    data = []
    for i in range(10):
        for metric in ['mae', 'r2', 'evs']:
            data.append({
                'Fold': f'Fold {i+1}',
                'Metric': metric.upper(),
                'Score': data_dict[metric][i],
                'Model': model_name
            })
    return pd.DataFrame(data)


vist_df = create_long_df(visl_res_dict, 'VISL')
lnr_df = create_long_df(lnr_res_dict, 'Linear Regression')
rf_df = create_long_df(rf_res_dict, 'Random Forest')

# concat all data frames together
all_df = pd.concat([vist_df, lnr_df, rf_df], ignore_index=True)


# compare models mean performance by grouping by metric (e.g., MAE) and then by model (e.g., VISL)
PRINT('Comparison of models performance:')
all_df.groupby(['Metric', 'Model'])['Score'].mean()


print(f'STATISTICS FORMAT (Mean, Varicance, STD)\n-----------------------------------------------------\nLiner Regression Statistics:\n{lnr_stats}\n\nRandom Forest Statistics:\n{rf_stats}\n\nVISL Model Statistics:\n{visl_evaluation_statistics}')








results = {'Linear Regression': lnr_stats, 'Random Forest': rf_stats, 'VISL': visl_evaluation_statistics}
plot_data = []

for model, stats in results.items():
    for metric, statistics_ls in stats.items():
        mean, var, std = statistics_ls
        plot_data.append({'Model': model, 'Metric': metric.upper(), 'Stat': 'Mean', 'Value': mean})

df_plot = pd.DataFrame(plot_data)

# barplot: Mean values
plt.figure(figsize=(10, 6))
sns.barplot(data=df_plot[df_plot['Stat'] == 'Mean'], x='Metric', y='Value', hue='Model')
plt.title('Mean Metric Comparison Across Models')
plt.ylabel('Score')
plt.legend(title='Model')
plt.tight_layout()
plt.show()



